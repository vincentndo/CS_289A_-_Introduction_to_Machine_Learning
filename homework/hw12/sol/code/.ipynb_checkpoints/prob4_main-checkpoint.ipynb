{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features ['pain', 'private', 'bank', 'money', 'drug', 'spam', 'prescription', 'creative', 'height', 'featured', 'differ', 'width', 'other', 'energy', 'business', 'message', 'volumes', 'revision', 'path', 'meter', 'memo', 'planning', 'pleased', 'record', 'out', 'semicolon', 'dollar', 'sharp', 'exclamation', 'parenthesis', 'square_bracket', 'ampersand']\n",
      "Train/test size (5172, 32) (5857, 32)\n",
      "\n",
      "Part 0: constant classifier\n",
      "Accuracy 0.709976798144\n",
      "\n",
      "Part (a-b): simplified decision tree\n",
      "Predictions [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0\n",
      " 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
      "\n",
      "========== Question 4.c ==========\n",
      "\n",
      "Titanic ====>\n",
      "Features [b'pclass' b'sex' b'age' b'sibsp' b'parch' b'ticket' b'fare' b'cabin'\n",
      " b'embarked']\n",
      "Train/test size (999, 9) (310, 9)\n",
      "\n",
      "Part (c): simplified decision tree - titanic\n",
      "Accuracy 0.8698698698698699\n",
      "\n",
      "Part (e): bagged - titanic\n",
      "Accuracy 0.8638638638638638\n",
      "\n",
      "Part (g): random forest - titanic\n",
      "Accuracy 0.6456456456456456\n",
      "\n",
      "Spam ====>\n",
      "Features ['pain', 'private', 'bank', 'money', 'drug', 'spam', 'prescription', 'creative', 'height', 'featured', 'differ', 'width', 'other', 'energy', 'business', 'message', 'volumes', 'revision', 'path', 'meter', 'memo', 'planning', 'pleased', 'record', 'out', 'semicolon', 'dollar', 'sharp', 'exclamation', 'parenthesis', 'square_bracket', 'ampersand']\n",
      "Train/test size (5172, 32) (5857, 32)\n",
      "\n",
      "Part (c): simplified decision tree - spam\n",
      "Accuracy 0.8002706883217324\n",
      "\n",
      "Part (e): bagged - spam\n",
      "Accuracy 0.8451276102088167\n",
      "\n",
      "Part (g): random forest - spam\n",
      "Accuracy 0.7099767981438515\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "\n",
    "eps = 1e-5  # a small number\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth=3, feature_labels=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        # TODO implement entropy function\n",
    "        n = y.size\n",
    "        hash_map = {}\n",
    "        for i in y:\n",
    "            hash_map[i] = 1 if i not in hash_map else hash_map[i] + 1\n",
    "            # if i not in hash_map:\n",
    "            #     hash_map[i] = 1\n",
    "            # else:\n",
    "            #     hash_map[i] += 1\n",
    "        ret = 0\n",
    "        for i in hash_map:\n",
    "            ret -= hash_map[i] / n * np.log(hash_map[i] / n)\n",
    "        return ret\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, thresh):\n",
    "        # TODO implement information gain function\n",
    "        # print(X)\n",
    "        index_0 = np.where(X < thresh)[0]\n",
    "        index_1 = np.where(X >= thresh)[0]\n",
    "        y0, y1 = y[index_0], y[index_1]\n",
    "        n0, n1 = y0.size, y1.size\n",
    "        return DecisionTree.entropy(y) - ( n0 / (n0 + n1) * DecisionTree.entropy(y0) + n1 / (n0 + n1) * DecisionTree.entropy(y1) )\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh):\n",
    "        idx0 = np.where(X[:,idx] < thresh)[0]\n",
    "        idx1 = np.where(X[:,idx] >= thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.max_depth > 0:\n",
    "            # compute entropy gain for all single-dimension splits,\n",
    "            # thresholding with a linear interpolation of 10 values\n",
    "            gains = []\n",
    "            thresh = np.array([np.linspace(np.min(X[:, i]) + eps,\n",
    "                                           np.max(X[:, i]) - eps, num=10) for i\n",
    "                               in range(X.shape[1])])\n",
    "            for i in range(X.shape[1]):\n",
    "                gains.append([self.information_gain(X[:, i], y, t) for t in\n",
    "                              thresh[i, :]])\n",
    "\n",
    "            gains = np.nan_to_num(np.array(gains))\n",
    "            self.split_idx, thresh_idx = np.unravel_index(np.argmax(gains),\n",
    "                                                          gains.shape)\n",
    "            self.thresh = thresh[self.split_idx, thresh_idx]\n",
    "            X0, y0, X1, y1 = self.split(X, y, idx=self.split_idx,\n",
    "                                        thresh=self.thresh)\n",
    "            if X0.size > 0 and X1.size > 0:\n",
    "                self.left = DecisionTree(max_depth=self.max_depth-1,\n",
    "                                         feature_labels=self.features)\n",
    "                self.left.fit(X0, y0)\n",
    "                self.right = DecisionTree(max_depth=self.max_depth-1,\n",
    "                                          feature_labels=self.features)\n",
    "                self.right.fit(X1, y1)\n",
    "            else:\n",
    "                self.max_depth = 0\n",
    "                self.data, self.labels = X, y\n",
    "                self.pred = stats.mode(y).mode[0]\n",
    "        else:\n",
    "            self.data, self.labels = X, y\n",
    "            self.pred = stats.mode(y).mode[0]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.max_depth == 0:\n",
    "            return self.pred * np.ones(X.shape[0])\n",
    "        else:\n",
    "            X0, idx0, X1, idx1 = self.split_test(X, idx=self.split_idx,\n",
    "                                                 thresh=self.thresh)\n",
    "            yhat = np.zeros(X.shape[0], dtype=\"int\")\n",
    "            yhat[idx0] = self.left.predict(X0)\n",
    "            yhat[idx1] = self.right.predict(X1)\n",
    "            return yhat\n",
    "\n",
    "\n",
    "class BaggedTrees(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, params=None, n=200):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.params = params\n",
    "        self.n = n\n",
    "        self.decision_trees = [\n",
    "            DecisionTreeClassifier(random_state=i, **self.params) for i in\n",
    "            range(self.n)]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        row_num, _ = X.shape\n",
    "        for i in range(self.n):\n",
    "            X_sampling = []\n",
    "            y_sampling = []\n",
    "            for _ in range(row_num):\n",
    "                index = randint(0, row_num - 1)\n",
    "                X_sampling.append(X[index])\n",
    "                y_sampling.append(y[index])\n",
    "            X_sampling = np.array(X_sampling)\n",
    "            y_sampling = np.array(y_sampling)\n",
    "            self.decision_trees[i].fit(X_sampling, y_sampling)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        ret = []\n",
    "        for i in range(self.n):\n",
    "            ret.append(self.decision_trees[i].predict(X))\n",
    "        ret = np.array(ret)\n",
    "        ret = np.mean(ret, axis=0)\n",
    "        array_round = np.vectorize(lambda x: int(round(x)))\n",
    "        return array_round(ret)\n",
    "\n",
    "\n",
    "class RandomForest(BaggedTrees):\n",
    "\n",
    "    def __init__(self, params=None, n=200, m=1):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        # TODO implement function\n",
    "        self.params = params\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.decision_trees = [\n",
    "            DecisionTreeClassifier(random_state=i, **self.params) for i in\n",
    "            range(self.n)]\n",
    "        self.features_list = []\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        row_num, feature_num = X.shape\n",
    "        for i in range(self.n):\n",
    "            self.features_list.append([randint(0, feature_num - 1) for _ in range(self.m)])\n",
    "            X_sampling = []\n",
    "            y_sampling = []\n",
    "            for _ in range(row_num):\n",
    "                index = randint(0, row_num - 1)\n",
    "                X_sampling.append(X[index, self.features_list[i]])\n",
    "                y_sampling.append(y[index])\n",
    "            X_sampling = np.array(X_sampling)\n",
    "            y_sampling = np.array(y_sampling)\n",
    "            self.decision_trees[i].fit(X_sampling, y_sampling)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        ret = []\n",
    "        for i in range(self.n):\n",
    "            ret.append(self.decision_trees[i].predict(X[:, self.features_list[i]]))\n",
    "        ret = np.array(ret)\n",
    "        ret = np.mean(ret, axis=0)\n",
    "        array_round = np.vectorize(lambda x: int(round(x)))\n",
    "        return array_round(ret)\n",
    "\n",
    "\n",
    "class BoostedRandomForest(RandomForest):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.ones(X.shape[0]) / X.shape[0]  # Weights on data\n",
    "        self.a = np.zeros(self.n)  # Weights on decision trees\n",
    "        # TODO implement function\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO implement function\n",
    "        pass\n",
    "\n",
    "\n",
    "def preprocess_titanic_data(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    df.drop([\"ticket\", \"cabin\"], axis=1, inplace=True)\n",
    "\n",
    "    row_indices = []\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.isnull(row).all():\n",
    "            row_indices.append(i)\n",
    "\n",
    "    df.drop(df.index[row_indices], inplace=True)\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        df.at[i, \"sex\"] = 0 if row[\"sex\"] == \"female\" else 1\n",
    "\n",
    "    df.at[df.age.isnull(), \"age\"] = df[\"age\"].mean()\n",
    "    df.at[df.fare.isnull(), \"fare\"] = df[df.pclass == 1][\"fare\"].mean()\n",
    "\n",
    "    df[\"e1\"], df[\"e2\"], df[\"e3\"] = [0, 0, 0]\n",
    "    for i, row in df.iterrows():\n",
    "        if row[\"embarked\"] == 'C':\n",
    "            df.at[i, \"e1\"] = 1\n",
    "        elif row[\"embarked\"] == 'Q':\n",
    "            df.at[i, \"e2\"] = 1\n",
    "        else:\n",
    "            df.at[i, \"e3\"] = 1\n",
    "\n",
    "    df.drop(\"embarked\", axis=1, inplace=True)\n",
    "\n",
    "    data_path_list = data_path.split('/')\n",
    "    data_path_list[-1] = \"preprocessed_\" + data_path_list[-1]\n",
    "    preprocessed_data_path = '/'.join(data_path_list)\n",
    "    df.to_csv(preprocessed_data_path, index=False)\n",
    "    return preprocessed_data_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = \"spam\"\n",
    "    params = {\n",
    "        \"max_depth\": 20,\n",
    "        # \"random_state\": 6,\n",
    "        \"min_samples_leaf\": 10,\n",
    "    }\n",
    "\n",
    "    if dataset == \"titanic\":\n",
    "        # Load titanic data\n",
    "        path_train = 'datasets/titanic/titanic_training.csv'\n",
    "        data = genfromtxt(path_train, delimiter=',', dtype=None)\n",
    "        features = data[0, 1:]  # features = all columns except survived\n",
    "        y = data[1:, 0]  # label = survived\n",
    "        class_names = [\"Died\", \"Survived\"]\n",
    "\n",
    "        # TODO implement preprocessing of Titanic dataset\n",
    "        preprocessed_training_path = preprocess_titanic_data(path_train)\n",
    "        path_test = 'datasets/titanic/titanic_testing_data.csv'\n",
    "        preprocessed_test_path = preprocess_titanic_data(path_test)\n",
    "\n",
    "        training_data = genfromtxt(preprocessed_training_path, delimiter=',')\n",
    "        test_data = genfromtxt(preprocessed_test_path, delimiter=',')\n",
    "        X, Z = training_data[1:, 1:], test_data[1:, :]\n",
    "        y = training_data[1:, 0]\n",
    "        y.astype(int)\n",
    "    elif dataset == \"spam\":\n",
    "        features = [\"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\",\n",
    "                    \"prescription\", \"creative\", \"height\", \"featured\", \"differ\",\n",
    "                    \"width\", \"other\", \"energy\", \"business\", \"message\",\n",
    "                    \"volumes\", \"revision\", \"path\", \"meter\", \"memo\", \"planning\",\n",
    "                    \"pleased\", \"record\", \"out\", \"semicolon\", \"dollar\", \"sharp\",\n",
    "                    \"exclamation\", \"parenthesis\", \"square_bracket\", \"ampersand\"]\n",
    "        assert len(features) == 32\n",
    "\n",
    "        # Load spam data\n",
    "        path_train = 'datasets/spam_data/spam_data.mat'\n",
    "        data = scipy.io.loadmat(path_train)\n",
    "        X = data['training_data']\n",
    "        y = np.squeeze(data['training_labels'])\n",
    "        Z = data['test_data']\n",
    "        class_names = [\"Ham\", \"Spam\"]\n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
    "\n",
    "    print(\"Features\", features)\n",
    "    print(\"Train/test size\", X.shape, Z.shape)\n",
    "\n",
    "    print(\"\\nPart 0: constant classifier\")\n",
    "    print(\"Accuracy\", 1 - np.sum(y) / y.size)\n",
    "\n",
    "    # Basic decision tree\n",
    "    print(\"\\nPart (a-b): simplified decision tree\")\n",
    "    dt = DecisionTree(max_depth=3, feature_labels=features)\n",
    "    dt.fit(X, y)\n",
    "    print(\"Predictions\", dt.predict(Z)[:100])\n",
    "    print()\n",
    "\n",
    "    # TODO implement and evaluate parts c-h\n",
    "\n",
    "    print(\"========== Question 4.c ==========\")\n",
    "\n",
    "    print()\n",
    "    print(\"Titanic ====>\")\n",
    "    dataset == \"titanic\"\n",
    "\n",
    "    path_train = 'datasets/titanic/titanic_training.csv'\n",
    "    data = genfromtxt(path_train, delimiter=',', dtype=None)\n",
    "    features = data[0, 1:]  # features = all columns except survived\n",
    "    y = data[1:, 0]  # label = survived\n",
    "    class_names = [\"Died\", \"Survived\"]\n",
    "\n",
    "    preprocessed_training_path = preprocess_titanic_data(path_train)\n",
    "    path_test = 'datasets/titanic/titanic_testing_data.csv'\n",
    "    preprocessed_test_path = preprocess_titanic_data(path_test)\n",
    "\n",
    "    training_data = genfromtxt(preprocessed_training_path, delimiter=',')\n",
    "    test_data = genfromtxt(preprocessed_test_path, delimiter=',')\n",
    "    X, Z = training_data[1:, 1:], test_data[1:, :]\n",
    "    y = training_data[1:, 0]\n",
    "    y.astype(int)\n",
    "\n",
    "    print(\"Features\", features)\n",
    "    print(\"Train/test size\", X.shape, Z.shape)\n",
    "    print()\n",
    "    print(\"Part (c): simplified decision tree - titanic\")\n",
    "    dt = DecisionTree(max_depth=10, feature_labels=features)\n",
    "    dt.fit(X, y)\n",
    "    y_predicted = dt.predict(X)\n",
    "    count = 0\n",
    "    for i, e in enumerate(y):\n",
    "        count += 1 if abs(y[i] - y_predicted[i]) < 0.01 else 0 \n",
    "    print(\"Accuracy\", count/y.size)\n",
    "    y_predicted = dt.predict(Z)\n",
    "    with open(\"submission_titanic_simpified.txt\", \"w\") as f:\n",
    "        for i in y_predicted:\n",
    "            f.write(str(i) + \"\\n\")\n",
    "\n",
    "    print()\n",
    "    print(\"Part (e): bagged - titanic\")\n",
    "    dt = BaggedTrees(params)\n",
    "    dt.fit(X, y)\n",
    "    y_predicted = dt.predict(X)\n",
    "    count = 0\n",
    "    for i, e in enumerate(y):\n",
    "        count += 1 if abs(y[i] - y_predicted[i]) < 0.01 else 0 \n",
    "    print(\"Accuracy\", count/y.size)\n",
    "    y_predicted = dt.predict(Z)\n",
    "    with open(\"submission_titanic_bagged.txt\", \"w\") as f:\n",
    "        for i in y_predicted:\n",
    "            f.write(str(i) + \"\\n\")\n",
    "\n",
    "    print()\n",
    "    print(\"Part (g): random forest - titanic\")\n",
    "    dt = RandomForest(params)\n",
    "    dt.fit(X, y)\n",
    "    y_predicted = dt.predict(X)\n",
    "    count = 0\n",
    "    for i, e in enumerate(y):\n",
    "        count += 1 if abs(y[i] - y_predicted[i]) < 0.01 else 0 \n",
    "    print(\"Accuracy\", count/y.size)\n",
    "    y_predicted = dt.predict(Z)\n",
    "    with open(\"submission_titanic_randomforest.txt\", \"w\") as f:\n",
    "        for i in y_predicted:\n",
    "            f.write(str(i) + \"\\n\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Spam ====>\")\n",
    "    dataset == \"spam\"\n",
    "\n",
    "    features = [\"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\",\n",
    "                    \"prescription\", \"creative\", \"height\", \"featured\", \"differ\",\n",
    "                    \"width\", \"other\", \"energy\", \"business\", \"message\",\n",
    "                    \"volumes\", \"revision\", \"path\", \"meter\", \"memo\", \"planning\",\n",
    "                    \"pleased\", \"record\", \"out\", \"semicolon\", \"dollar\", \"sharp\",\n",
    "                    \"exclamation\", \"parenthesis\", \"square_bracket\", \"ampersand\"]\n",
    "    assert len(features) == 32\n",
    "\n",
    "    # Load spam data\n",
    "    path_train = 'datasets/spam_data/spam_data.mat'\n",
    "    data = scipy.io.loadmat(path_train)\n",
    "    X = data['training_data']\n",
    "    y = np.squeeze(data['training_labels'])\n",
    "    y.astype(int)\n",
    "    Z = data['test_data']\n",
    "    class_names = [\"Ham\", \"Spam\"]\n",
    "\n",
    "    print(\"Features\", features)\n",
    "    print(\"Train/test size\", X.shape, Z.shape)\n",
    "    print()\n",
    "    print(\"Part (c): simplified decision tree - spam\")\n",
    "    dt = DecisionTree(max_depth=5, feature_labels=features)\n",
    "    dt.fit(X, y)\n",
    "    y_predicted = dt.predict(X)\n",
    "    count = 0\n",
    "    for i, e in enumerate(y):\n",
    "        count += 1 if abs(y[i] - y_predicted[i]) < 0.01 else 0 \n",
    "    print(\"Accuracy\", count/y.size)\n",
    "    y_predicted = dt.predict(Z)\n",
    "    with open(\"submission_spam_simpified.txt\", \"w\") as f:\n",
    "        for i in y_predicted:\n",
    "            f.write(str(i) + \"\\n\")\n",
    "\n",
    "    print()\n",
    "    print(\"Part (e): bagged - spam\")\n",
    "    dt = BaggedTrees(params)\n",
    "    dt.fit(X, y)\n",
    "    y_predicted = dt.predict(X)\n",
    "    count = 0\n",
    "    for i, e in enumerate(y):\n",
    "        count += 1 if abs(y[i] - y_predicted[i]) < 0.01 else 0 \n",
    "    print(\"Accuracy\", count/y.size)\n",
    "    y_predicted = dt.predict(Z)\n",
    "    with open(\"submission_spam_bagged.txt\", \"w\") as f:\n",
    "        for i in y_predicted:\n",
    "            f.write(str(i) + \"\\n\")\n",
    "\n",
    "    print()\n",
    "    print(\"Part (g): random forest - spam\")\n",
    "    dt = RandomForest(params)\n",
    "    dt.fit(X, y)\n",
    "    y_predicted = dt.predict(X)\n",
    "    count = 0\n",
    "    for i, e in enumerate(y):\n",
    "        count += 1 if abs(y[i] - y_predicted[i]) < 0.01 else 0 \n",
    "    print(\"Accuracy\", count/y.size)\n",
    "    y_predicted = dt.predict(Z)\n",
    "    with open(\"submission_spam_randomforest.txt\", \"w\") as f:\n",
    "        for i in y_predicted:\n",
    "            f.write(str(i) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
